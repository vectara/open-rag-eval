input_queries: "queries.csv"  # file with a list of queries to use for evaluation

# Evaluation results are written to the "results_folder" folder.
# Specify the names of file where the answers, results and metrics will be generated.
results_folder: "results/"
generated_answers: "answers_1.csv"
eval_results_file: "results_1.csv"
metrics_file: "metrics.png"

evaluator:
  type: "TRECEvaluator"  
  model:
    type: "OpenAIModel"
    name: "gpt-4o-mini"
    api_key: ${oc.env:OPENAI_API_KEY}  # Reads from environment variable.

connector:
  type: "VectaraConnector"
  options:
    api_key: ${oc.env:VECTARA_API_KEY}
    corpus_key: "fiqa"
    query_config:
      search:
        lexical_interpolation: 0.005
        limit: 100
        context_configuration:
          sentences_before: 2
          sentences_after: 2
          start_tag: "<em>"
          end_tag: "</em>"
        reranker:
          - type: "chain"
            rerankers:
              - type: "customer_reranker"
                reranker_name: "Rerank_Multilingual_v1"
                limit: 50
              - type: "mmr"
                diversity_bias: 0.01
                limit: 20
      generation:
        generation_preset_name: "vectara-summary-table-md-query-ext-jan-2025-gpt-4o"
        max_used_search_results: 5
        citations: {"style": "numeric"}
        response_language: "eng"
        # prompt_template: "sample_prompt.txt"   - Optional
        enable_factual_consistency_score: False
      intelligent_query_rewriting: False
      save_history: False
