# Query Generation Configuration for Local Files
# This example shows how to generate queries from local text files

# Document source configuration
document_source:
  type: "LocalFileSource"
  options:
    path: "/path/to/documents"      # Replace with your documents directory
    file_extensions: [".txt", ".md"]
    min_doc_size: 500               # Minimum document size in characters
    max_num_docs: null              # null = use all documents

# LLM model for query generation
model:
  type: "OpenAIModel"
  name: "gpt-4o-mini"
  api_key: ${oc.env:OPENAI_API_KEY}

# Alternative: Use Anthropic Claude
# model:
#   type: "AnthropicModel"
#   name: "claude-sonnet-4-5"
#   api_key: ${oc.env:ANTHROPIC_API_KEY}

# Alternative: Use Google Gemini
# model:
#   type: "GeminiModel"
#   name: "gemini-2.5-flash"
#   api_key: ${oc.env:GEMINI_API_KEY}

# Query generation parameters
generation:
  n_questions: 100                 # Total queries to generate
  min_words: 5                     # Minimum words per query (generates diverse lengths)
  max_words: 25                    # Maximum words per query
  questions_per_doc: 10            # Max queries per document

  # Question type distribution (weights are auto-normalized)
  # Set any weight to 0 to disable that question type
  question_types:
    directly_answerable: 25        # Questions answerable directly from text
    reasoning_required: 25         # Questions requiring reasoning/inference
    unanswerable: 25               # Questions not answerable from text
    partially_answerable: 25       # Questions partially answerable from text

  # Example: Disable unanswerable questions
  # question_types:
  #   directly_answerable: 50
  #   reasoning_required: 30
  #   unanswerable: 0
  #   partially_answerable: 20

# Output configuration
output:
  format: "csv"                    # or "jsonl"
  base_filename: "queries"         # Generates queries.csv
  include_metadata: false
